% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}
\include{use}
\begin{document}

\title{Development and implementation of an OpenCL driven neural network for use in Python}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor Heinrich Dinkel\\
       \affaddr{1140339107}
       \email{heinrich.dinkel@gmail.com}
% 2nd. author
\alignauthor Daniel Laidig\\
       \affaddr{J114030910}
       \email{daniel@laidig.info}
% 3rd. author
\alignauthor Christian WÃ¼rthner\\
       \email{c.wuerthner@me.com}
}

\date{30 July 1999}
\maketitle
\begin{abstract}
This paper describes the development and implementation process of a neural network which is driven by OpenCL using GPU acceleration. The neural network should be usable within Python using a wrapper for the shared object file of the C++ library.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Computer Science}{Neural Networks}

\terms{Implementation}

\keywords{Neural Network, OpenCL, Python, C++11, Back-propagation, Stochastic Gradient Descent} % NOT required for Proceedings



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
Artificial neural networks (ANNs)are processing devices (algorithms or actual hardware) that are loosely modeled after the neuronal structure of the human cerebral cortex but on much smaller scales. A large ANN might have hundreds or thousands of processor units, whereas a mamalian brain has billions of neurons with a corresponding increase in magnitude of their overall interaction and emergent behavior. Although ANN researchers are generally not concerned with whether their networks accurately resemble biological systems, some have. For example, researchers have accurately simulated the function of the retina and modeled the eye rather well.

Neural neworks are typically organized in layers. Layers are made up of a number of interconnected \textbf{nodes} which contain an \textbf{activation function}. Patterns are presented to the network via the input layer, which communicates to one or more \textbf{hidden layers} where the actual processing is done via a system of weighted \textbf{connections}. The hidden layers then link to an \textbf{output layer}.

Most ANNs contain some form of \textbf{learning rule} which modifies the weights of the connections according to the input patterns that it is presented with. In a sense, ANNs learn by example as do their biological counterparts; a child learns to recognize dogs from examples of dogs.

Although there are many different kinds of learning rules used by neural networks, this paper is concerned only with one; the delta rule. The delta rule is often utilized by the most common class of ANNs called \textbf{backpropagational neural networks} (BPNNs). Backpropagation is an abbreviation for the backwards propagation of error.

With the delta rule, as with other types of backpropagation, learning is a supervised process that occurs with each cycle or \textbf{epoch} (i.e. each time the network has processed the full dataset once) through a forward activation flow of outputs, and the backwards error propagation of weight adjustments. More simply, when a neural network is initially presented with a pattern it makes a random guess as to what it might be. It then sees how far its answer was from the actual one and makes an appropriate adjustment to its connection weights.

\section{Prelimitaries}


%Can put  this into the introduction
We use the following notation:
\begin{itemize}
\item Matrices are written in uppercase bold e.g. $\mathbf{X}$.
\item Vectors are written in lowercase bold e..g $\mathbf{x}$.
\item scalars are written in lowercase or uppercase. Lowercase indicates that it is a counting variable and uppercase that it is one of the limits in an finite set.
\item For all functions , e.g. $f(x)$, where $x = \mathbf{X}$ , we apply the function element wise.
\item Dot product is indicated by simple concatenation of two matrices/vectors e.g. $\mathbf{X} \mathbf{y}$.
\item Element wise multiplication is indicated as $\mathbf{X} \otimes \mathbf{Z}$.
\end{itemize}

Neural networks are a statistical model for regression or classification. Even though naturally they do not produce any classification output, we can model the regressed output to be used as a classifier.

Overall we have two different modes: Training and evaluation. In the training phase we use the back propagation algorithm to train the network and adjust it so that it produces output, which is close to our target output.

Furthermore a neural network contains $L$ hidden layers, which are called hidden since their output is not directly observable.

The weights are in our simple case randomly initialized and then updated using the back propagation rule.

A basic neural network has therefore the following parameters:
\begin{itemize}
\item The weights from an neuron $k$ in layer $a$ to a neuron $j$ in layer $b$
\item The input to the network, usually a vector $\mathbf{x}$ with $k$ values
\item The target which should be estimated, usually a vector $\mathbf{t}$ with $q$ values.
\item The learning rate $\eta$, which is the indicator how fast the network learns.
\end{itemize}

Moreover we can add some advanced techniques as the momentum to speed up the training the momentum


\subsection{Gradient Descent}
Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, it takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent \cite{Kiwiel2001,Qian1999}.

Gradient descent is based on the observation that if the multivariable function $F(\mathbf{x})$ is defined and differentiable in a neighborhood of a point $\mathbf{a}$, then $F(\mathbf{x})$ decreases fastest if one goes from $\mathbf{a}$ in the direction of the negative gradient of F at $\mathbf{a}, -\nabla F(\mathbf{a})$ \cite{Yuan1999}. It follows that, if
\begin{equation}
\mathbf{b} = \mathbf{a}-\gamma\nabla F(\mathbf{a})
\end{equation}
for $\gamma$ small enough, then $F(\mathbf{a})\geq F(\mathbf{b})$. With this observation in mind, one starts with a guess $\mathbf{x}_0$ for a local minimum of F, and considers the sequence $\mathbf{x}_0, \mathbf{x}_1, \mathbf{x}_2, \dots$ such that
\begin{equation}
\mathbf{x}_{n+1}=\mathbf{x}_n-\gamma_n \nabla F(\mathbf{x}_n),\ n \ge 0.
\end{equation}

Finally we have \cite{Cauchy1847}:
\begin{equation}
F(\mathbf{x}_0)\ge F(\mathbf{x}_1)\ge F(\mathbf{x}_2)\ge \cdots,
\end{equation}


\subsection{Minibatch Stochastic Gradient Descent}

Both statistical estimation and machine learning consider the problem of minimizing an objective function that has the form of a sum:
\begin{equation}
Q(w) = \sum_{i=1}^n Q_i(w)
\end{equation}

where the parameter $w^*$ which minimizes Q(w) is to be estimated. Each summand function $Q_i$ is typically associated with the $i$-th observation in the data set (used for training).

When used to minimize the above function, a standard (or "batch") gradient descent method would perform the following iterations :
\begin{equation}
w := w - \eta \nabla Q(w) = w - \alpha \sum_{i=1}^n \nabla Q_i(w)
\end{equation}
where $\eta$ is the learning rate.

But since in many cases, the update of one gradient would take to estimate the gradient over the whole dataset, the computation would take too much time. Often times we only need a rough estimate of the gradient, not a precise one.

What stochastic gradient descent does is to remove the sum and only uses one sample to estimate the gradient.
\begin{equation}
w := w - \eta \nabla Q(w) = w - \alpha \nabla Q(w)
\end{equation}
In practice this turns out to be less effective than batch gradient descent, since we only get a very rough estimate of the gradient.
Therefore the minibatch gradient descent technique is used. The difference here is that while batch gradient descent uses $n$ samples and stochastic gradient descent uses only $1$, minibatch gradient uses $b$ training samples, where $b << n$. Typical sizes for $b$ are in the range if $b \in {2,\ldots,100}$.

Therefore in minibatch stochastic Gradient descent we randomly subset the training set and only take some samples out of it and estimate the gradient out of the seen sample set.

The important thing to do is to randomize the training set, since otherwise only a small portion of the training set will be seen and leads to wrong gradients.

\subsection{The Forward Backward procedure in detail}

To train the network we use the standard back-propagation algorithm, which essentially does a forward and a backward pass of the network in a row.
Firstly we initialize the weights in a matrix.
\begin{align}
\mathbf{W}_{j \times k } = \begin{bmatrix}
w_{11} & w_{12} & \ldots & w_{1k}\\
w_{21} & \ddots & & \vdots\\
\vdots & & \ddots  & \vdots\\
w_{j1} & \ldots & \ldots  & w_{jk}\\
\end{bmatrix},
\mathbf{x}_{1 \times k} = \begin{bmatrix}
a_{1} \\
\vdots\\
\vdots \\
a_{k}
\end{bmatrix}
\end{align}


\paragraph{Forward propagation} is one of the two passes the network needs to do ( hence it's name ).
In forward propagation the network calculates the predicted output of the network.

The output of the hidden layer $j$ of the network can be calculated as the weighted sum of the inputs
\begin{align}
\mathbf{nnet}_j = \sum_{k=1}^{n}w_{kj}x_k = \mathbf{W} \mathbf{x}
\end{align}

Even though this method should already work, we add a bias to every node to increase the learning speed and void that the network stops learning.

\begin{align}
\mathbf{nnet}_j = \sum_{k=1}^{n}w_{kj}x_k + b_k = \mathbf{W} \mathbf{x} + \mathbf{b}
\end{align}r


Later we see that back-propagation needs some variables, which can be already precomputed during the forward step. These two variables are the output $\mathbf{o}_j$ of layer $j$ and the derivative of the output w.r.t to the weighted sum $\mathbf{nnet}_j$. We represent the derivatives in a matrix $\mathbf{D}$ and the output of the current layer
\begin{align}
\mathbf{D}_j = \frac{\mathbf{o}_j}{\mathbf{nnet}_j}
\end{align}
In case of sigmoid activation function, we obtain:
\begin{align}
\mathbf{D}_j = \varphi \left( \mathbf{nnet}_j \right) \left( 1 - \varphi \left(\mathbf{nnet}_j \right) \right)
\end{align}
For the outputs, we simply feed forward the network and store the output $\mathbf{o}_j$ in out buffered lists.
\begin{align}
\mathbf{o}_j = \varphi \left( \mathbf{Wx+b} \right)
\end{align}

As soon as the feed forward is done, we produced an output of the network, which we can denote as $\mathbf{o}_L$. Now we need to decide how close our output is to the targetted output $\mathbf{t}$. For this we use a cost function in usual cases it is sufficient to use $MSE$ as such, which is defined as:
\begin{equation}
\label{eq:mse}
\mathbf{MSE} = \frac{1}{2} \left( \mathbf{o}_L - \mathbf{t} \right) ^2
\end{equation}

Here (\ref{eq:mse}) we need to make sure that the last output layer has the same dimensions as the target output.


\paragraph{Backpropagation} starts when all layers are trained. The idea behind back propagation is that the hidden layers do not produce any output. So we cannot modify their weights directly, since we donât know how large the error was. With back propagation we calculate the error at the output layer $L$ and then propagate this error to the hidden layers back. Therefore we update all $L-1$ weight matrices and biases.

At first we calculate the differences between the target output and the estimated output.

\begin{align}
\mathbf{err} &= (\mathbf{o}_L - \mathbf{t})\\
\boldsymbol{\delta}_L &= \mathbf{err} \otimes \mathbf{D}_L\\
\nabla \mathbf{W}_L &= \mathbf{o}_L \boldsymbol{\delta}_L^T
\end{align}

We store both, the deltas and the nablas to later update the weights ( with the $\nabla$s) and the biases (using the $\mathbf{\delta}$).
From here on we then calculate the other layers backwards, we do:
\begin{align}
\boldsymbol{\delta}_i &= \mathbf{D}_i \otimes \left( \mathbf{W}_{i+1}^T \boldsymbol{\delta}_{i+1} \right)\\
\nabla \mathbf{W}_i &= \mathbf{o}_i \boldsymbol{\delta}_i^T
\end{align}
Whereas we again store the deltas and the nablas to later update the weights.

To update the weights, we use the usual gradient descent update rule:
\begin{align}
\mathbf{W}^{*} = \mathbf{W} - \alpha \nabla \mathbf{W}_i^T\\
\end{align}

To update the biases, we use essentially the same update rule, but only consider the given $\boldsymbol{\delta}_{i}$s.
\begin{equation}
\mathbf{b}^{*} = \mathbf{b} - \alpha \boldsymbol{\delta}_i^T\\
\end{equation}

Finally, if we would like to improve the converging speed we can apply momentum. Momentum adds extra "velocity" towards the gradient curve, by using the last estimated value of the gradient $\mathbf{W}^{*}_{i-1}$ ($i$ denotes the current iteration) and applying on that the momentum ($ \alpha$) as:

\begin{equation}
\mathbf{W}_{i+1} = \mathbf{W}_i - \eta \nabla \mathbf{W}_i + \alpha \mathbf{W}_i
\end{equation}

The momentum is initialized with 0 and takes effect after one iteration of gradient descent.

\section{Implementation details}

OpenCL offers multiple implementation types. It is natively written in C, but has also a C++ wrapper onboard. We used in our project the C++ wrapper since it is more naturally to use that in a C++ project.
First one needs to include the necessary header into any class.

\begin{lstlisting}[caption=OpenCL C++ header]
#include <CL/cl.hpp>
\end{lstlisting}


\paragraph{The interface to OpenCL} was written in C++11 and makes heavily use of the current variadic args feature. For this small scale task, the interface is definitely too complex, but it can be used for any other OpenCL task.

\label{lst:mult}
\begin{lstlisting}[caption=Example Kernel function]
__kernel void mult(const int wSrc, __global const float* A,__global const float* B,__global float* output)
{
   const int idx = get_global_id(0);
   const int idy = get_global_id(1);

   output[idy*wSrc+idx] = A[idy*wSrc+idx] * B[idy*wSrc+idx];
}
\end{lstlisting}
OpenCL uses externally defined functions ( coined as kernels ) to compile the code and run it, during the runtime. Therefore one needs to write a kernel for its needs. An example kernel can be seen at \ref{lst:mult}. In our case we defined various kernels, for dot products and other matrix operations.

Since OpenCL is a multi device and platform GPU/CPU interface, one needs to first figure out which kind of platform (e.g. NVIDIA,AMD,Intel) the current machine is using and then decide which accelerator will be used \ref{lst:plat}.

\label{lst:plat}
\begin{lstlisting}[caption=Get Platforms and devices]
void exampleplatform(const char * programpath){
std::vector<cl::Platform> all_platforms;
cl::Platform::get(&all_platforms);
//Assume having only one platform
cl::Platform defaultplatform = all_platforms[0];
std::vector<cl::Device> all_gpu_devices;
//CL_DEVICE_TYPE_GPU can also be CL_DEVICE_TYPE_CPU for CPU
defaultplatform.getDevices(CL_DEVICE_TYPE_GPU, &all_gpu_devices);
//Assume having only one device / take the first
cl::Device device = all_gpu_devices.front();
//Init the current context with the device
cl::Context(device);
//We wrote a helper function here to get the string data out of the kernel file
const char* content = util::file_contents(programpath);
}
\end{lstlisting}


Moreover since OpenCL is compiled during runtime, one needs to extract the code from the kernel file (e.g. kernel.cl) and give this content to the OpenCL.

As soon as the context is initialized we can run our kernels on the device. The problem here is that the kernel is defined with it's own parameters and types, but we dont know these during the compile time.
To have a universal interface we used the variadicargs feature to allow the programmer a very straight forward way to use any kernel function.
\label{lst:kernel}
\begin{lstlisting}[caption=Kernel usage]
void runKernel(const char *kernelname){
cl::Program::Sources sources;
//contents is the already read out content of the kernels file (e.g. kernels.cl)
sources.push_back(std::make_pair(contents,strlen(contents)+1));
//Init the program with the context and the source
cl::Program program(context,sources);
//Build the program on the device
program.build({device});
cl::CommandQueue queue(context,device);
//The operator allows us to set args to the kernel
cl::Kernel kernel_operator(program,kernelname);
//kernel_operator allows us to send arguments to the kernel by calling
//kernel_operator.setArgs(ARGNUMBER,ARGUMENT);

//Init space on the device using cl::Buffers
//Send Arguments to the device ....
// quene.enqueueWriteBuffer() .....
//Wait until the arguments did arrive on the device
queue.finish();

//Execute!
cl::Event event;
queue.enqueueNDRangeKernel(kernel_operator,cl::NullRange,cl::NDRange(10),cl::NullRange,NULL,&event);
//Wait until the execution has finished
event.wait();
// Read out the results by calling
// quene.enqueueReadBuffer()
}
\end{lstlisting}

The usual Kernel execution can be seen in \ref{lst:kernel}. Even though it is recommended to use OpenCL in this fashion, we cannot cope with different arguments for the kernel. Therefore we would need to init a device and context every time ( or at least as a singleton ) and then rewrite the argument passing for every parameter independently. This would take a lot of time, if the GPU is used extensively.

Our solution to that problem is as follows:

\begin{lstlisting}[caption=Add arguments to kernel dynamic way]
class OpenCL{

//Constructor ..... etc
//	Hook for the iteration
template<std::size_t P=0,typename... Tp>
	typename std::enable_if<P == sizeof...(Tp), void>::type addkernelargs(std::tuple<Tp ...>&& t,cl::Kernel &k,cl::CommandQueue &,std::vector<cl::Buffer> &outputbuffers) const{
	// Do nothing
	}

//  Start of the iteration
template<std::size_t P = 0, typename... Tp>
	typename std::enable_if< P < sizeof...(Tp), void>::type addkernelargs(std::tuple<Tp...> && t,cl::Kernel &kernel,cl::CommandQueue &,std::vector<cl::Buffer> &outputbuffers) const{
		 // Type
        typedef typename std::tuple_element<P, std::tuple<Tp...>>::type type;

        // Add the value of the current item from std::get<P> to the args in kernel
        // This function decides which type the kernel arg is
        addkernelarg(P, std::get<P>(t), kernel,queue,outputbuffers);

        // Recurse to get the remaining args
        addkernelargs<P + 1, Tp...>(std::forward<std::tuple<Tp...>>(t), kernel,queue,outputbuffers);
	}

//	Adding Std::vector as type to the kernel args list
	template<typename T>
	void addkernelarg(std::size_t i, std::vector<T> const & arg, cl::Kernel & kernel,cl::CommandQueue &) const{
		cl::Buffer buffer(this->context,CL_MEM_READ_WRITE,arg.size()*sizeof(T));
		queue.enqueueWriteBuffer(buffer,CL_FALSE,0,sizeof(T)*arg.size(),&(arg[0]));
		kernel.setArg(i,buffer);

	}


//	Adding any array into the kernel args
	template<typename T,std::size_t N>
	void addkernelarg(std::size_t i, T const (& arg)[N], cl::Kernel & kernel,cl::CommandQueue &) const{
		cl::Buffer buffer(this->context,CL_MEM_READ_WRITE,N*sizeof(T));
		queue.enqueueWriteBuffer(buffer,CL_FALSE,0,sizeof(T)*N,&arg);
		kernel.setArg(i,buffer);
	}

	// Adding any constant to the kernel
	template<typename T>
	void addkernelarg(std::size_t i, T const & arg, cl::Kernel & kernel,cl::CommandQueue &) const{
		cl::Buffer buffer(this->context,CL_MEM_READ_WRITE,arg.size()*sizeof(T));
		queue.enqueueWriteBuffer(buffer,CL_FALSE,0,sizeof(T)*arg.size(),&(arg[0]));
		kernel.setArg(i,buffer);
	}

}
\end{lstlisting}


\subsection{Transparently handling OpenCL within a Matrix class}

To be able to use high level C++ code to implement the feedforward and backpropagation algorithm, we introduced a CL\_Matrix class, which provides common matrix operation like multiplication, addition and so on. Most matrix operations are implemented as OpenCL kernels on the GPU. To avoid unnecessary transfers between the RAM and the GPU memory, the matrix class implements a state handling. Two methods for this are shown below, \texttt{syncToRam()} and \texttt{moveToRam()} work in a similar way.

\begin{lstlisting}[caption=State handling to allow the Matrix data to be both used in RAM and in the GPU memory]
template<typename T>
inline void CL_Matrix<T>::syncToGpu() const {
    if (this->state == OnlyRam) {
        this->gpu_buf = this->_cl.putDataOnGPU(this->mat);
        this->state = Synced;
    }

}

template<typename T>
inline void CL_Matrix<T>::moveToGpu() {
    this->syncToGpu();
    this->state = OnlyGpu;

}
\end{lstlisting}

This way, we can both perform OpenCL operations and access the data from the normal C++ code, for example to get the result. Data transfers are avoided if possible, so for example multiple consecutive OpenCL operations can be performed without transfering data.




This recipe can be used to do the same actions for the reading buffers out after the transfer has finished.
Therefore we can create a highly dynamic wrapper class for OpenCL.

\subsection{Use a C++ Class in Python}
To use a C++ class in a Python program, Python's ctypes library can be used to invoke functions on a shared object file. The class will be wrapped into a Python class which uses the ctypes library to invoke functions on the C++ object. As the ctypes library is not able to handle C++ objects directly, a wrapper function must be written in C for every function of the C++ class. These plain C functions can then be invoked by ctypes.

A simple example of such a C++ class is seen in the following figure:

\begin{lstlisting}[caption=Example C++ class for wrapping a C++ object in a Python object (Example.cpp)]
class WrapperExample {
    std::string* name;

public:
    WrapperExample(std::string name) {
        this->name = new std::string(name);
    }
    ~WrapperExample() {
        delete this->name;
    }
    void sayHello() {
        std::cout << "Hello " << *(this->name) << "!" << std::endl;
    }
};

extern "C" {
    WrapperExample* WrapperExample_new(char* name) {
        return new WrapperExample(name);
    }
    void WrapperExample_sayHello(WrapperExample* e) {
        e->sayHello();
    }
    void WrapperExample_delete(WrapperExample* e) {
        delete e;
    }
}
\end{lstlisting}

To compile the given C++ code into a shared object file which can be used by Python's ctypes library, following compilation statements can be used:

\lstset{language= bash}
\begin{lstlisting}[caption=Compilation of the C++ class into a shared object file]
g++ -std=c++11 -c -fPIC Example.cpp -o example.o
g++ -std=c++11 -shared -Wl,-soname,example.so -o example.so  example.o
\end{lstlisting}

The Python wrapper will call the functions defined in the \texttt{extern} section. For every function in the \texttt{extern} section, the Python class will have one function to wrap it. The function \texttt{WrapperExample\_new()} will be called in the constructor of the Python class and \texttt{WrapperExample\_delete()} in its  destructor. All functions besides the \texttt{WrapperExample\_new()} function receive a pointer to an instance of the C++ class \texttt{WrapperExample} on which the function should be invoked. These objects are created in \texttt{WrapperExample\_new()} and the pointers to them are stored by Python.

The corresponding Python class to wrap the C++ class shown above looks like this:

\lstset{language=Python}
\begin{lstlisting}[caption=Python class wrapping the C++ class shown above (Example.py)]
from ctypes import cdll

lib = cdll.LoadLibrary('example.so')

class ExampleWrapper:
    def __init__(self, name):
        self.obj = lib.WrapperExample_new(name)

    def sayHello(delf):
        lib.WrapperExample_sayHello(self.obj)

    def __del__(self):
        lib.WrapperExample_delete(self.obj)
\end{lstlisting}

The Python class \texttt{ExampleWrapper} can now be used as any other Python class.

\subsection{Use a C++ Class in Python}
To use a C++ class in a Python program, Python's ctypes library can be used to invoke functions on a shared object file. The class will be wrapped into a Python class which uses the ctypes library to invoke functions on the C++ object. As the ctypes library is not able to handle C++ objects directly, a wrapper function must be written in C for every function of the C++ class. These plain C functions can then be invoked by ctypes.

A simple example of such a C++ class is seen in the following figure:

\begin{lstlisting}[caption=Example C++ class for wrapping a C++ object in a Python object (Example.cpp)]
class WrapperExample {
    std::string* name;
    
public:
    WrapperExample(std::string name) {
        this->name = new std::string(name);
    }
    ~WrapperExample() {
        delete this->name;
    }
    void sayHello() {
        std::cout << "Hello " << *(this->name) << "!" << std::endl;
    }
};

extern "C" {
    WrapperExample* WrapperExample_new(char* name) {
        return new WrapperExample(name);
    }
    void WrapperExample_sayHello(WrapperExample* e) {
        e->sayHello();
    }
    void WrapperExample_delete(WrapperExample* e) {
        delete e;
    }
}
\end{lstlisting}

To compile the given C++ code into a shared object file which can be used by Python's ctypes library, following compilation statements can be used:

\lstset{language= bash}
\begin{lstlisting}[caption=Compilation of the C++ class into a shared object file]
g++ -std=c++11 -c -fPIC Example.cpp -o example.o
g++ -std=c++11 -shared -Wl,-soname,example.so -o example.so  example.o
\end{lstlisting}

The Python wrapper will call the functions defined in the \texttt{extern} section. For every function in the \texttt{extern} section, the Python class will have one function to wrap it. The function \texttt{WrapperExample\_new()} will be called in the constructor of the Python class and \texttt{WrapperExample\_delete()} in its  destructor. All functions besides the \texttt{WrapperExample\_new()} function receive a pointer to an instance of the C++ class \texttt{WrapperExample} on which the function should be invoked. These objects are created in \texttt{WrapperExample\_new()} and the pointers to them are stored by Python.

The corresponding Python class to wrap the C++ class shown above looks like this:

\lstset{language=Python}
\begin{lstlisting}[caption=Python class wrapping the C++ class shown above (Example.py)]
from ctypes import cdll

lib = cdll.LoadLibrary('example.so')

class ExampleWrapper:
    def __init__(self, name):
        self.obj = lib.WrapperExample_new(name)
        
    def sayHello(delf):
        lib.WrapperExample_sayHello(self.obj)
        
    def __del__(self): 
        lib.WrapperExample_delete(self.obj)
\end{lstlisting}

The Python class \texttt{ExampleWrapper} can now be used as any other Python class.

\subsection{Using the Interface}
The Python class wrapping our \texttt{NeuralNetwork} class can be found in \texttt{NeuralNetwork.py}. To create a new \texttt{NeuralNetwork} the following constructor can be used:

\lstset{language=Python}
\begin{lstlisting}[caption=Constructor to create a neural network]
nn = NeuralNetwork(layerCount=3,
                   layerSize=np.array([784, 1000, 10]),
                   actFunctions=np.array([1, 1]),)
\end{lstlisting}

In this case a neural network with 3 layers is created. The list \texttt{layerSize} contains the size of every layer. The neural network created in this example has an input layer with 784 nodes, one hidden layer with 1000 nodes and a output layer with 10 nodes. The activation functions are defined by the list \texttt{actFunctions}. The value 0 will set the activation function TAN\_H and the value 1 SIGMOID. As the input layer has no activation function, the size of the list \texttt{actFunctions} is only 2. The sizes of the input and output layers can later be queried by the functions \texttt{getInputSize()} and \texttt{getOutputSize()}.

To train the neural network, the functions \texttt{trainBatch()} and \texttt{trainStochastic()} can be used. Both functions expect two matrices as parameters. The first matrix defines the input values which will be given to the neural network and the second parameter defines the expected output values. One row in one in the matrices is one train case for the neural network. The functions return a matrix with the errors. \texttt{trainStochastic()} uses stochastic gradient descent. The learning rate and momentum can be defined by the corresponding parameters \texttt{learningRate} and \texttt{momentum} and the number of epochs can be defined. An example usage can be seen in the following listing:

\lstset{language=Python}
\begin{lstlisting}[caption=Usage of trainsgd() and train()]
errors = nn.train(trainImages[:, 0:20], trainOutput[:, 0:20], learningRate=0.2, momentum=0.1, numEpochs=10000)
print errors

errors = nn.trainsgd(trainImages[:, 0:20], trainOutput[:, 0:20], learningRate=0.2, momentum=0.1, numEpochs=10000)
print errors
\end{lstlisting}


The function \texttt{test()} can be used to test a input value with the neural network. In difference to \texttt{train()} only one matrix is expected as parameter. The values from the matrix will be given to the neural network. One column of the matrix is one test case. The function will return a matrix with the value of the output nodes for every test case.

\lstset{language=Python}
\begin{lstlisting}[caption=Usage of test()]
result = nn.test(testImages[:, 0:20])
print result
\end{lstlisting}


The current state of the neural network including the weight biases and the configuration can be dumped into a file using the \texttt{save()} function. The path to the file must be supplied as a parameter. To restore the neural network later, following constructor can be used:
\lstset{language=Python}
\begin{lstlisting}[caption=Constructor to load a dump file]
nn.save("path/to/savefile.bin")

nn2 = NeuralNetwork(saveFile="path/to/savefile.bin");
\end{lstlisting}

\section{Experiments}

In our experiments we used the MNIST database for digit recognition.

MNIST consists of 60000 data samples of recognized written digits, stored as greyscale images.
Ever one of these samples has a size of $28 \times 28$ pixels. The digits which can be recognized range from zero to nine.

Therefore we have an input layer size of $28 \times 28= 784$ and $10$ output nodes representing each number respectively.

\begin{table}[h]
\centering
\caption{Network configuration}
\label{tab:config}
\begin{tabular}{rr|r|r|r}
\hline
configname & layers & neurons/layer& l-rate & momentum\\
\hline
small & 1 & 1024 & 0.1 to 0.7 & 0.01 \\
\hline
middle & 2 & 1024 & 0.1 to 0.7 & 0.01 \\
\hline
big & 3 & 1024 & 0.1 to 0.7 & 0.01\\
\hline
\end{tabular}
\end{table}
We used the networks shown in \ref{tab:config}, where we tried to use different starting learning rates to achieve better results.
All out networks use \textbf{10000 epochs} and \textbf{stochastic gradient descent} as its learning algorithm.


The results can be seen in the table \ref{tab:correct}. 
\begin{table}[h]
\centering
\caption{Correct results ( in \%) depending on the learning rate}
\label{tab:correct}
\begin{tabular}{r|r|r|r|r|r|r|r}
& 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 \\
\hline
small & 89 & 92 & \textbf{96} & 94 & 90 & 89 & 89 \\
\hline
mid & 91 &85 & 86 & 81 & 86 & 85 & 92 \\
\hline
big & 84 & 87 & 77 & 56 & 66 & 11 & 64  \\
\end{tabular}
\end{table}

Finally we can see the performance of our neural network. We want to emphasize that we used Intel HD4000 GPU's, which are not very powerful GPU's. Fortunately one could easily run this code with any other GPU, since OpenCL supports every large GPU manufacture product.

\begin{table}[h]
\centering
\caption{Training time ( in sec )}
\label{tab:traintime}
\begin{tabular}{r|r}
& time \\
\hline
small & 287 \\
\hline
mid & 530 \\
\hline
big &  700 \\
\end{tabular}
\end{table}

\section{Conclusion}

As we can see in \ref{tab:correct}, the results seem to be somewhat inconsistent. This can be explained by knowing that often times a larger neural network abstracts a (maybe simple) correlation between the inputs so much that it decorrelates the input with the output. In our example we can see that digit recognitions work well if only a few layers are used.
Moreover, random weights can lead the network to converge to a local minimum which is not necessary the global one.

Many other techniques to increase the performance could be implemented such as Restricted Boltzmann machines (RMBs) to cope with the random initialization of the network.

In our experiments we only used the checker-board algorithm to calculate the dot products, but there are more efficient ways to do so. Moreover loop unrolling is a powerful method to calculate multiple instructions at once, as long as the dataset has a certain size to enable the unroll.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\bibliographystyle{abbrv}
\bibliography{sigproc}

\balancecolumns
% That's all folks!

\end{document}
